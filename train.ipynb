{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c184fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from actor_critic import ActorCritic\n",
    "from world_model import MiniWorldModel\n",
    "from vq_vae import Encoder, vq_vae\n",
    "from safetensors.torch import load_file\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6495ab6",
   "metadata": {},
   "source": [
    "### Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48be52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/miniconda3/envs/circle/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained weights\n",
    "encoder_state = load_file(\"pretrained/encoder.safetensors\")\n",
    "vq_state = load_file(\"pretrained/vq.safetensors\")\n",
    "world_model_state = torch.load(\"checkpoints/best_model_step_46500.pth\")\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "encoder.load_state_dict(encoder_state)\n",
    "\n",
    "vq_vae = vq_vae.to(device)\n",
    "vq_vae.load_state_dict(vq_state)\n",
    "\n",
    "world_model = MiniWorldModel(num_actions=18, num_games=6).to(device)\n",
    "world_model.load_state_dict(world_model_state)\n",
    "\n",
    "actor_critic = ActorCritic(num_actions=18, max_seq_len=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d5f169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embed\n",
      "blocks\n",
      "blocks.layers\n",
      "blocks.layers.0\n",
      "blocks.layers.0.self_attn\n",
      "blocks.layers.0.self_attn.out_proj\n",
      "blocks.layers.0.linear1\n",
      "blocks.layers.0.dropout\n",
      "blocks.layers.0.linear2\n",
      "blocks.layers.0.norm1\n",
      "blocks.layers.0.norm2\n",
      "blocks.layers.0.dropout1\n",
      "blocks.layers.0.dropout2\n",
      "blocks.layers.1\n",
      "blocks.layers.1.self_attn\n",
      "blocks.layers.1.self_attn.out_proj\n",
      "blocks.layers.1.linear1\n",
      "blocks.layers.1.dropout\n",
      "blocks.layers.1.linear2\n",
      "blocks.layers.1.norm1\n",
      "blocks.layers.1.norm2\n",
      "blocks.layers.1.dropout1\n",
      "blocks.layers.1.dropout2\n",
      "blocks.layers.2\n",
      "blocks.layers.2.self_attn\n",
      "blocks.layers.2.self_attn.out_proj\n",
      "blocks.layers.2.linear1\n",
      "blocks.layers.2.dropout\n",
      "blocks.layers.2.linear2\n",
      "blocks.layers.2.norm1\n",
      "blocks.layers.2.norm2\n",
      "blocks.layers.2.dropout1\n",
      "blocks.layers.2.dropout2\n",
      "blocks.layers.3\n",
      "blocks.layers.3.self_attn\n",
      "blocks.layers.3.self_attn.out_proj\n",
      "blocks.layers.3.linear1\n",
      "blocks.layers.3.dropout\n",
      "blocks.layers.3.linear2\n",
      "blocks.layers.3.norm1\n",
      "blocks.layers.3.norm2\n",
      "blocks.layers.3.dropout1\n",
      "blocks.layers.3.dropout2\n",
      "blocks.layers.4\n",
      "blocks.layers.4.self_attn\n",
      "blocks.layers.4.self_attn.out_proj\n",
      "blocks.layers.4.linear1\n",
      "blocks.layers.4.dropout\n",
      "blocks.layers.4.linear2\n",
      "blocks.layers.4.norm1\n",
      "blocks.layers.4.norm2\n",
      "blocks.layers.4.dropout1\n",
      "blocks.layers.4.dropout2\n",
      "blocks.layers.5\n",
      "blocks.layers.5.self_attn\n",
      "blocks.layers.5.self_attn.out_proj\n",
      "blocks.layers.5.linear1\n",
      "blocks.layers.5.dropout\n",
      "blocks.layers.5.linear2\n",
      "blocks.layers.5.norm1\n",
      "blocks.layers.5.norm2\n",
      "blocks.layers.5.dropout1\n",
      "blocks.layers.5.dropout2\n",
      "actor_head\n",
      "actor_head.0\n",
      "actor_head.1\n",
      "critic_head\n",
      "critic_head.0\n",
      "critic_head.1\n"
     ]
    }
   ],
   "source": [
    "for name, module in actor_critic.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a983bc7",
   "metadata": {},
   "source": [
    "### Add lora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b08ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 448,528 || all params: 5,719,313 || trainable%: 7.8423\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension (adjust 8-32 based on compute)\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\n",
    "        \"out_proj\",  # Attention output Linear\n",
    "        \"linear1\",  # FFN first Linear\n",
    "        \"linear2\",  # FFN second Linear\n",
    "        \"obs_head.1\",  # obs_head Linear\n",
    "        \"reward_head.1\",  # reward_head Linear\n",
    "    ],  # Use '*' wildcard for layers 0-5\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",  # Don't adapt biases (optional, but efficient)\n",
    "    modules_to_save=[\n",
    "        \"obs_embed\",\n",
    "        \"action_embed\",\n",
    "        \"game_embed\",\n",
    "    ],  # Fully train/save embeddings\n",
    ")\n",
    "\n",
    "world_model = get_peft_model(world_model, lora_config)\n",
    "world_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be983d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
