{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893d684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdejayvu\u001b[0m (\u001b[33mdejayvu-university-of-oxford\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jay/workspace/circle/wandb/run-20250714_041556-qyynagus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model/runs/qyynagus' target=\"_blank\">Multigames-50k</a></strong> to <a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model' target=\"_blank\">https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model/runs/qyynagus' target=\"_blank\">https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model/runs/qyynagus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Subset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    ")\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import trange\n",
    "import zarr\n",
    "from world_model import MiniWorldModel\n",
    "from vq_vae import Encoder, vq_vae\n",
    "from task_embed import TaskEmbed\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"pretrain-world-model\",\n",
    "    name=\"Multigames-50k\",\n",
    "    config={\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 1000,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"horizon\": 8,\n",
    "    },\n",
    ")\n",
    "\n",
    "encoder_state = load_file(\"pretrained/encoder.safetensors\")\n",
    "vq_state = load_file(\"pretrained/vq.safetensors\")\n",
    "decoder_state = load_file(\"pretrained/decoder.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db781280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTrajectoryDataset(Dataset):\n",
    "    def __init__(self, games, horizon=8):\n",
    "        self.root = zarr.open_group(\"dataset50k.zarr\", mode=\"r\")\n",
    "        self.games = games\n",
    "        self.horizon = horizon\n",
    "        self.data = []\n",
    "\n",
    "        for gid, game in enumerate(self.games):\n",
    "            frames = self.root[game][\"frames\"][:]\n",
    "            actions = self.root[game][\"actions\"][:]\n",
    "            rewards = self.root[game][\"rewards\"][:]\n",
    "            dones = self.root[game][\"dones\"][:]\n",
    "\n",
    "            for idx in trange(len(frames) - horizon):\n",
    "                # check if any done in the horizon (to avoid crossing episode boundary)\n",
    "                if dones[idx : idx + horizon].any():\n",
    "                    continue\n",
    "\n",
    "                frame_seq = frames[idx : idx + horizon, -1]  # (H, 3, 84, 84)\n",
    "                action_seq = actions[idx : idx + horizon]  # (H,)\n",
    "                reward_seq = rewards[idx : idx + horizon]  # (H,)\n",
    "\n",
    "                self.data.append((frame_seq, action_seq, reward_seq, gid))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_seq, action_seq, reward_seq, gid = self.data[idx]\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(frame_seq).float().div_(255),  # (H, 3, 84, 84)\n",
    "            torch.from_numpy(action_seq).long(),  # (H,)\n",
    "            torch.from_numpy(reward_seq).float(),  # (H,)\n",
    "            torch.tensor(gid, dtype=torch.long),  # game id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8974f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/miniconda3/envs/circle/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Task embedding\n",
    "task_embed = TaskEmbed(num_known_tasks=6).to(device)\n",
    "\n",
    "# Initialize models\n",
    "world_model = MiniWorldModel(num_actions=18, task_embed=task_embed).to(device)\n",
    "\n",
    "# Load pretrained weights\n",
    "encoder = Encoder().to(device)\n",
    "quantizer = vq_vae.to(device)\n",
    "encoder.load_state_dict(encoder_state)\n",
    "quantizer.load_state_dict(vq_state)\n",
    "\n",
    "world_model.train()\n",
    "encoder.eval()\n",
    "quantizer.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    world_model.parameters(), lr=1e-4, weight_decay=1e-5\n",
    ")\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d15c5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49992/49992 [00:00<00:00, 516755.09it/s]\n",
      "100%|██████████| 49992/49992 [00:00<00:00, 512908.58it/s]\n",
      "100%|██████████| 49992/49992 [00:00<00:00, 519464.00it/s]\n",
      "100%|██████████| 49992/49992 [00:00<00:00, 462822.31it/s]\n",
      "100%|██████████| 49992/49992 [00:00<00:00, 512343.37it/s]\n",
      "100%|██████████| 49992/49992 [00:00<00:00, 515515.11it/s]\n"
     ]
    }
   ],
   "source": [
    "games = [\n",
    "    \"SpaceInvaders\",\n",
    "    \"Krull\",\n",
    "    \"BeamRider\",\n",
    "    \"Hero\",\n",
    "    \"StarGunner\",\n",
    "    \"MsPacman\",\n",
    "]\n",
    "dataset = MultiTrajectoryDataset(games, horizon=8)\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "indices = np.arange(len(dataset))\n",
    "perm = rng.permutation(indices)\n",
    "split = int(len(perm) * 0.9)\n",
    "train_indices = perm[:split]\n",
    "val_indices = perm[split:]\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "\n",
    "batch_size = 384\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=RandomSampler(train_subset, replacement=True),\n",
    "    num_workers=8,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SequentialSampler(val_subset),\n",
    "    num_workers=8,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd9c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.75\n",
    "gamma = 10.0\n",
    "\n",
    "\n",
    "def validate(global_step, reward_weight):\n",
    "    obs_loss_sum = 0.0\n",
    "    reward_loss_sum = 0.0\n",
    "    token_cnt = 0\n",
    "    tp = pos = fp = 0  # 整体\n",
    "\n",
    "    val_game_obs_loss_sum = {gid: 0.0 for gid in range(6)}\n",
    "    val_game_reward_loss_sum = {gid: 0.0 for gid in range(6)}\n",
    "    val_game_token_cnt = {gid: 0 for gid in range(6)}\n",
    "    val_game_tp = {gid: 0 for gid in range(6)}\n",
    "    val_game_pos = {gid: 0 for gid in range(6)}\n",
    "    val_game_fp = {gid: 0 for gid in range(6)}\n",
    "\n",
    "    val_bar = tqdm(\n",
    "        val_loader,\n",
    "        leave=False,\n",
    "        desc=f\"Validating at step {global_step}\",\n",
    "    )\n",
    "\n",
    "    world_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for frames, actions, rewards, game_ids in val_bar:\n",
    "            B, T, C, Ht, Wt = frames.shape\n",
    "\n",
    "            frames = frames.view(-1, 3, 84, 84).to(\n",
    "                device, dtype=torch.float32, non_blocking=True\n",
    "            )\n",
    "            actions = actions.to(device, dtype=torch.long, non_blocking=True)\n",
    "            rewards = rewards.to(\n",
    "                device, dtype=torch.float32, non_blocking=True\n",
    "            )\n",
    "            game_ids = game_ids.to(device)\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                # 移动到 autocast 内\n",
    "                z_e = encoder(frames)\n",
    "                _, indices, _ = quantizer(z_e)\n",
    "                obs_tokens = indices.view(B, T, 16)\n",
    "\n",
    "                pred_obs_logits, pred_rewards = world_model(\n",
    "                    obs_tokens, actions, game_ids\n",
    "                )\n",
    "                pred_obs_logits = pred_obs_logits.permute(0, 1, 3, 2)\n",
    "\n",
    "                obs_loss = F.cross_entropy(\n",
    "                    pred_obs_logits.reshape(-1, 512),\n",
    "                    obs_tokens.reshape(-1),\n",
    "                    reduction=\"mean\",\n",
    "                )\n",
    "                reward_target = (rewards.abs() > 1e-6).float()\n",
    "                reward_loss = sigmoid_focal_loss(\n",
    "                    pred_rewards,\n",
    "                    reward_target,\n",
    "                    reduction=\"mean\",\n",
    "                    alpha=alpha,\n",
    "                    gamma=gamma,\n",
    "                )\n",
    "\n",
    "            batch_tokens = obs_tokens.numel()\n",
    "            obs_loss_sum += obs_loss.item() * batch_tokens\n",
    "            reward_loss_sum += reward_loss.item() * batch_tokens\n",
    "            token_cnt += batch_tokens\n",
    "\n",
    "            # 分组计算 losses 和 tp/fp\n",
    "            unique_gids = game_ids.unique()\n",
    "            for g in unique_gids:\n",
    "                mask = game_ids == g\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # Slice\n",
    "                obs_tokens_g = obs_tokens[mask]\n",
    "                pred_obs_logits_g = pred_obs_logits[mask]\n",
    "                rewards_g = rewards[mask]\n",
    "                pred_rewards_g = pred_rewards[mask]\n",
    "\n",
    "                # Group losses (用全局 gamma)\n",
    "                obs_loss_g = F.cross_entropy(\n",
    "                    pred_obs_logits_g.permute(0, 1, 3, 2).reshape(-1, 512),\n",
    "                    obs_tokens_g.reshape(-1),\n",
    "                    reduction=\"mean\",\n",
    "                )\n",
    "                reward_target_g = (rewards_g.abs() > 1e-6).float()\n",
    "                reward_loss_g = sigmoid_focal_loss(\n",
    "                    pred_rewards_g,\n",
    "                    reward_target_g,\n",
    "                    reduction=\"mean\",\n",
    "                    alpha=alpha,\n",
    "                    gamma=gamma,  # 统一到 5.0\n",
    "                )\n",
    "\n",
    "                tokens_g = obs_tokens_g.numel()\n",
    "                gid = g.item()\n",
    "                val_game_obs_loss_sum[gid] += obs_loss_g.item() * tokens_g\n",
    "                val_game_reward_loss_sum[gid] += (\n",
    "                    reward_loss_g.item() * tokens_g\n",
    "                )\n",
    "                val_game_token_cnt[gid] += tokens_g\n",
    "\n",
    "                # 分组计算 tp/fp (替换原 per-sample)\n",
    "                prob_g = torch.sigmoid(pred_rewards_g)\n",
    "                pred_g = prob_g > 0.5\n",
    "                target_g = reward_target_g.bool()  # 用 bool 以匹配\n",
    "                val_game_tp[gid] += (pred_g & target_g).sum().item()\n",
    "                val_game_pos[gid] += target_g.sum().item()\n",
    "                val_game_fp[gid] += (pred_g & (~target_g)).sum().item()\n",
    "\n",
    "            # 整体 tp/fp (保持)\n",
    "\n",
    "            prob = torch.sigmoid(pred_rewards)\n",
    "            pred = prob > 0.5\n",
    "            tp += (pred & reward_target.bool()).sum().item()\n",
    "            pos += reward_target.sum().item()\n",
    "            fp += (pred & (~reward_target.bool())).sum().item()\n",
    "\n",
    "        # 移动到循环外：计算 avg 和 log\n",
    "        for gid in range(6):\n",
    "            if val_game_token_cnt[gid] > 0:\n",
    "                obs_loss_game = (\n",
    "                    val_game_obs_loss_sum[gid] / val_game_token_cnt[gid]\n",
    "                )\n",
    "                reward_loss_game = (\n",
    "                    val_game_reward_loss_sum[gid] / val_game_token_cnt[gid]\n",
    "                )\n",
    "                weighted_reward_loss_game = reward_loss_game * reward_weight\n",
    "                total_loss_game = obs_loss_game + weighted_reward_loss_game\n",
    "                recall_game = (\n",
    "                    val_game_tp[gid] / val_game_pos[gid]\n",
    "                    if val_game_pos[gid]\n",
    "                    else float(\"nan\")\n",
    "                )\n",
    "                precision_game = (\n",
    "                    val_game_tp[gid] / (val_game_tp[gid] + val_game_fp[gid])\n",
    "                    if (val_game_tp[gid] + val_game_fp[gid])\n",
    "                    else float(\"nan\")\n",
    "                )\n",
    "                f1_game = (\n",
    "                    (\n",
    "                        2\n",
    "                        * precision_game\n",
    "                        * recall_game\n",
    "                        / (precision_game + recall_game)\n",
    "                    )\n",
    "                    if precision_game and recall_game\n",
    "                    else float(\"nan\")\n",
    "                )\n",
    "\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        f\"val/game_{games[gid]}/obs_loss\": obs_loss_game,\n",
    "                        f\"val/game_{games[gid]}/raw_reward_loss\": reward_loss_game,  # 区分 raw\n",
    "                        f\"val/game_{games[gid]}/weighted_reward_loss\": weighted_reward_loss_game,\n",
    "                        f\"val/game_{games[gid]}/total_loss\": total_loss_game,\n",
    "                        f\"val/game_{games[gid]}/recall\": recall_game,\n",
    "                        f\"val/game_{games[gid]}/precision\": precision_game,\n",
    "                        f\"val/game_{games[gid]}/f1\": f1_game,\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "        obs_loss_epoch = obs_loss_sum / token_cnt\n",
    "        reward_loss_epoch = reward_loss_sum / token_cnt\n",
    "        weighted_reward_loss_epoch = reward_loss_epoch * reward_weight\n",
    "        total_loss_epoch = obs_loss_epoch + weighted_reward_loss_epoch\n",
    "\n",
    "        recall = tp / pos if pos else float(\"nan\")\n",
    "        precision = tp / (tp + fp) if (tp + fp) else float(\"nan\")\n",
    "        f1 = (\n",
    "            (2 * precision * recall / (precision + recall))\n",
    "            if precision and recall\n",
    "            else float(\"nan\")\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"all_val/obs_loss\": obs_loss_epoch,\n",
    "                \"all_val/raw_reward_loss\": reward_loss_epoch,  # 区分\n",
    "                \"all_val/weighted_reward_loss\": weighted_reward_loss_epoch,\n",
    "                \"all_val/total_loss\": total_loss_epoch,\n",
    "                \"all_val/recall\": recall,\n",
    "                \"all_val/precision\": precision,\n",
    "                \"all_val/f1\": f1,\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "\n",
    "        scheduler.step(reward_loss_epoch)\n",
    "\n",
    "    return total_loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82427400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01:  38%|███▊      | 261/696 [06:37<10:55,  1.51s/it, loss=9.94, obs_loss=4.74, reward_loss=5.2] "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "patience = 3  # 连续3次val不改善就stop\n",
    "early_stop_counter = 0\n",
    "min_delta = 0.0001  # 最小改善阈值\n",
    "\n",
    "wandb.watch(world_model, log=\"gradients\", log_freq=100)\n",
    "scaler = GradScaler()\n",
    "global_step = 0\n",
    "\n",
    "initial_reward_weight = 70000.0  # 初始值，根据日志调优\n",
    "decay_rate = 0.99  # 每 epoch 衰减率\n",
    "min_reward_weight = 1.0  # 最小值，防止过低\n",
    "\n",
    "for epoch in range(1000):\n",
    "    bar = tqdm(train_loader, leave=True, desc=f\"Epoch {epoch + 1:02d}\")\n",
    "    world_model.train()\n",
    "    for frames, actions, rewards, game_ids in bar:\n",
    "        global_step += 1\n",
    "\n",
    "        B, T, C, Ht, Wt = frames.shape\n",
    "\n",
    "        frames = frames.view(-1, 3, 84, 84).to(\n",
    "            device, dtype=torch.float32, non_blocking=True\n",
    "        )\n",
    "        actions = actions.to(device, dtype=torch.long, non_blocking=True)\n",
    "        rewards = rewards.to(device, dtype=torch.float32, non_blocking=True)\n",
    "        game_ids = game_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z_e = encoder(frames)\n",
    "            _, indices, _ = quantizer(z_e)\n",
    "            obs_tokens = indices.view(B, T, 16)\n",
    "\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            pred_obs_logits, pred_rewards = world_model(\n",
    "                obs_tokens, actions, game_ids\n",
    "            )\n",
    "\n",
    "            pred_obs_logits = pred_obs_logits.permute(0, 1, 3, 2)\n",
    "\n",
    "            obs_loss = F.cross_entropy(\n",
    "                pred_obs_logits.reshape(-1, 512),\n",
    "                obs_tokens.reshape(-1),\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "\n",
    "            reward_target = (rewards.abs() > 1e-6).float()\n",
    "            reward_loss = sigmoid_focal_loss(\n",
    "                pred_rewards,\n",
    "                reward_target,\n",
    "                reduction=\"mean\",\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "\n",
    "            reward_weight = max(\n",
    "                initial_reward_weight * (decay_rate**epoch), min_reward_weight\n",
    "            )\n",
    "            weighted_reward_loss = reward_loss * reward_weight\n",
    "            loss = obs_loss + weighted_reward_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            world_model.parameters(), max_norm=1.0, norm_type=2.0\n",
    "        )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                \"train/obs_loss\": obs_loss.item(),\n",
    "                \"train/reward_loss\": weighted_reward_loss.item(),\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/grad_norm\": grad_norm.item(),\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "\n",
    "        bar.set_postfix(\n",
    "            obs_loss=obs_loss.item(),\n",
    "            reward_loss=weighted_reward_loss.item(),\n",
    "            loss=loss.item(),\n",
    "        )\n",
    "\n",
    "    total_loss_epoch = validate(global_step, reward_weight)\n",
    "\n",
    "    if total_loss_epoch < best_val_loss - min_delta:\n",
    "        best_val_loss = total_loss_epoch\n",
    "        early_stop_counter = 0\n",
    "        checkpoint_path = os.path.join(\n",
    "            checkpoint_dir, f\"best_model_step_{global_step}.pth\"\n",
    "        )\n",
    "        torch.save(world_model.state_dict(), checkpoint_path)\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at step {global_step}\")\n",
    "            break\n",
    "\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
