{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893d684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "from einops import rearrange\n",
    "from safetensors.torch import load_file\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "from tqdm import tqdm\n",
    "\n",
    "encoder_state = load_file(\"encoder.safetensors\")\n",
    "vq_state = load_file(\"vq.safetensors\")\n",
    "decoder_state = load_file(\"decoder.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8ddd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.act(self.net(x))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    (B, 3, 84, 84) → (B, 16, 512)\n",
    "\n",
    "    - 4 conv layers\n",
    "    - 2 residual blocks per layer\n",
    "    - 21x21 feature map partitioned into 4x4 non-overlapping 5x5 windows -> 16 tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=512):\n",
    "        super().__init__()\n",
    "        self.patch = 5\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64),\n",
    "        )  # 84 → 42\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "        )  # 42 → 21\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.SiLU(),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "        )  # 21 → 21\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.SiLU(),\n",
    "            ResidualBlock(out_channels),\n",
    "            ResidualBlock(out_channels),\n",
    "        )  # 21 → 21\n",
    "\n",
    "        self.projection = nn.Linear(\n",
    "            self.patch * self.patch * out_channels, out_channels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        # (B, 512, 21, 21)\n",
    "        x = x[:, :, 0:20, 0:20]\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
    "            p1=self.patch,\n",
    "            p2=self.patch,\n",
    "        )  # (B, 16, 5*5*512)\n",
    "\n",
    "        x = self.projection(x)  # (B, 16, 512)\n",
    "\n",
    "        return x  # (B, 16, 512)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    (B, 16, 512) → (B, 3, 84, 84)\n",
    "\n",
    "    - reverse of Encoder\n",
    "    - 16 tokens -> 4x4 grid of 5x5 patches -> (B, 512, 21, 21)\n",
    "    - 4 deconv layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=512, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.patch = 5\n",
    "\n",
    "        self.projection = nn.Linear(\n",
    "            in_channels, self.patch * self.patch * in_channels\n",
    "        )\n",
    "\n",
    "        self.deconv4 = nn.Sequential(\n",
    "            ResidualBlock(in_channels),\n",
    "            ResidualBlock(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1),\n",
    "        )  # 21 → 21\n",
    "\n",
    "        self.deconv3 = nn.Sequential(\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "        )  # 21 → 21\n",
    "\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "        )  # 21 → 42\n",
    "\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, out_channels, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "        )  # 42 → 84\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 16, 512)\n",
    "        x = self.projection(x)  # (B, 16, 5×5×512)\n",
    "        x = x.view(\n",
    "            x.size(0), 4, 4, 512, self.patch, self.patch\n",
    "        )  # (B, 4, 4, 512, 5, 5)\n",
    "        x = x.permute(0, 3, 1, 4, 2, 5)  # (B, 512, 4, 5, 4, 5)\n",
    "        x = x.reshape(x.size(0), 512, 4 * 5, 4 * 5)  # (B, 512, 20, 20)\n",
    "\n",
    "        # 填充到 21×21（补 1 行 1 列）\n",
    "        x = F.pad(x, (0, 1, 0, 1))  # (B, 512, 21, 21)\n",
    "\n",
    "        x = self.deconv4(x)  # (B, 256, 21, 21)\n",
    "        x = self.deconv3(x)  # (B, 128, 21, 21)\n",
    "        x = self.deconv2(x)  # (B, 64, 42, 42)\n",
    "        x = self.deconv1(x)  # (B, 3, 84, 84)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193e71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniWorldModel(nn.Module):\n",
    "    def __init__(self, num_actions=18):\n",
    "        super().__init__()\n",
    "        self.obs_embed = nn.Embedding(512, 256)  # 512 tokens, 256 dim\n",
    "        self.action_embed = nn.Embedding(\n",
    "            num_actions, 256\n",
    "        )  # num_actions tokens, 256 dim\n",
    "\n",
    "        self.pos = nn.Parameter(\n",
    "            torch.randn(1, 1024, 256)\n",
    "        )  # positional encoding for 64 tokens\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=256,\n",
    "            nhead=4,\n",
    "            dim_feedforward=1024,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "        self.obs_head = nn.Sequential(\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, 512),  # output obs embedding\n",
    "        )\n",
    "\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        nn.init.zeros_(self.reward_head[-1].weight)\n",
    "        nn.init.zeros_(self.reward_head[-1].bias)\n",
    "\n",
    "    def rollout(self, obs_tokens, action_tokens, actor_critic):\n",
    "        \"\"\"\n",
    "        obs_tokens: token ids from VQ-VAE -> (B, 16)\n",
    "        action_tokens: token ids for actions -> (B,)\n",
    "        actor_critic: ActorCritic model to compute next state and reward\n",
    "        returns: next_obs_logits, reward\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, obs_tokens, action_tokens):\n",
    "        \"\"\"\n",
    "        obs_tokens: (B, T, K)  ← each obs is K tokens\n",
    "        action_tokens: (B, T)  ← each timestep has 1 discrete action\n",
    "        Returns:\n",
    "            pred_obs_logits: (B, T, K, vocab_size)\n",
    "            pred_rewards:    (B, T)\n",
    "        \"\"\"\n",
    "        B, T, K = obs_tokens.shape\n",
    "\n",
    "        # flatten for embedding\n",
    "        z = obs_tokens.reshape(B, T * K)  # (B, T*K)\n",
    "        a = action_tokens  # (B, T)\n",
    "\n",
    "        # embed tokens\n",
    "        z_embed = self.obs_embed(z)  # (B, T*K, 256)\n",
    "        a_embed = self.action_embed(a).unsqueeze(2)  # (B, T, 1, 256)\n",
    "\n",
    "        # interleave\n",
    "        tokens = torch.cat(\n",
    "            [z_embed.view(B, T, K, -1), a_embed], dim=2\n",
    "        )  # (B, T, K+1, 256)\n",
    "        tokens = tokens.reshape(B, T * (K + 1), 256)  # (B, T*(K+1), 256)\n",
    "\n",
    "        # add positional encoding\n",
    "        pos_embed = self.pos[:, : tokens.size(1), :]  # (1, T*(K+1), 256)\n",
    "        x = tokens + pos_embed  # (B, T*(K+1), 256)\n",
    "\n",
    "        # causal mask (L, L)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(x.size(1)).to(\n",
    "            x.device\n",
    "        )  # (T*(K+1), T*(K+1))\n",
    "\n",
    "        # decode\n",
    "        out = self.transformer(x, mask, is_causal=True)  # (B, T*(K+1), 256)\n",
    "\n",
    "        out = out.reshape(B, T, K + 1, 256)  # (B, T, K+1, 256)\n",
    "\n",
    "        obs_logits = self.obs_head(out[:, :, :-1, :])  # (B, T, K, 512)\n",
    "        rewards = self.reward_head(out[:, :, -1, :]).squeeze(-1)  # (B, T)\n",
    "\n",
    "        return obs_logits, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611595a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_actions=18):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(512, 512)\n",
    "        self.pos = nn.Parameter(torch.randn(1, 16, 512))\n",
    "\n",
    "        self.blocks = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=512,\n",
    "                nhead=8,\n",
    "                activation=\"gelu\",\n",
    "                dim_feedforward=1024,\n",
    "                batch_first=True,\n",
    "                norm_first=True,\n",
    "            ),\n",
    "            num_layers=6,\n",
    "        )\n",
    "\n",
    "        self.actor_head = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, num_actions),  # num_actions\n",
    "        )\n",
    "\n",
    "        self.critic_head = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: token ids from VQ-VAE -> (B, 16)\n",
    "        returns:\n",
    "        - action logits: (B, 18)\n",
    "        - value: (B, 1)\n",
    "        \"\"\"\n",
    "        x = self.embed(x) + self.pos[:, : x.size(1), :]\n",
    "        x = self.blocks(x)\n",
    "        x = x.mean(dim=1)  # (B, 512)\n",
    "\n",
    "        action_logits = self.actor_head(x)\n",
    "        value = self.critic_head(x)\n",
    "\n",
    "        return action_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db781280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zarr\n",
    "\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, game, horizon=8):\n",
    "        self.root = zarr.open_group(\"dataset200k.zarr\", mode=\"r\")\n",
    "        self.frames = self.root[game][\"frames\"][:]  # (N, 4, 3, 84, 84)\n",
    "        self.actions = self.root[game][\"actions\"][:]  # (N,)\n",
    "        self.rewards = self.root[game][\"rewards\"][:]  # (N,)\n",
    "        self.dones = self.root[game][\"dones\"][:]  # (N,)\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames) - self.horizon\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # check if any done in the horizon (to avoid crossing episode boundary)\n",
    "        if self.dones[idx : idx + self.horizon].any():\n",
    "            # skip invalid episode (you can also loop until valid one)\n",
    "            return self.__getitem__((idx + self.horizon) % len(self))\n",
    "\n",
    "        frame_seq = self.frames[idx : idx + self.horizon, -1]  # (H, 3, 84, 84)\n",
    "        action_seq = self.actions[idx : idx + self.horizon]  # (H,)\n",
    "        reward_seq = self.rewards[idx : idx + self.horizon]  # (H,)\n",
    "        reward_seq = (reward_seq - reward_seq.mean()) / (\n",
    "            reward_seq.std() + 1e-6\n",
    "        )  # normalize rewards\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(frame_seq).float().div_(255),  # (H, 3, 84, 84)\n",
    "            torch.from_numpy(action_seq),  # (H,)\n",
    "            torch.from_numpy(reward_seq).float(),  # (H,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df274c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/miniconda3/envs/circle/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder().to(\"cuda\")\n",
    "quantizer = VectorQuantize(\n",
    "    dim=512,\n",
    "    codebook_size=512,  # each table smaller\n",
    "    decay=0.8,\n",
    "    commitment_weight=0.1,\n",
    ").to(\"cuda\")\n",
    "\n",
    "encoder.load_state_dict(encoder_state)\n",
    "quantizer.load_state_dict(vq_state)\n",
    "world_model = MiniWorldModel(num_actions=6).to(\"cuda\")\n",
    "\n",
    "world_model.train()\n",
    "encoder.eval()\n",
    "quantizer.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(world_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d15c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Subset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    ")\n",
    "\n",
    "dataset = TrajectoryDataset(\"SpaceInvaders\", horizon=8)\n",
    "N = len(dataset)  # 起点的总个数，≈ 帧数 - horizon\n",
    "h = dataset.horizon\n",
    "dones = dataset.dones  # (N_total,)\n",
    "\n",
    "# 1) 找到每个episode结束的位置\n",
    "episode_ends = np.where(dones)[0]  # 包含终止帧本身\n",
    "episode_starts = np.insert(episode_ends[:-1] + 1, 0, 0)\n",
    "\n",
    "# 2) 为每条 episode 生成“合法起点”区间 [start, end - h]\n",
    "episode_start_idx = []\n",
    "for s, e in zip(episode_starts, episode_ends):\n",
    "    valid = np.arange(s, max(s, e - h + 1))  # 可能为空，但不会跨边界\n",
    "    if len(valid):\n",
    "        episode_start_idx.append(valid)\n",
    "\n",
    "# 3) 按 episode 随机打乱后 9:1 切分\n",
    "rng = np.random.default_rng(seed=42)\n",
    "perm = rng.permutation(len(episode_start_idx))\n",
    "split = int(len(perm) * 0.9)\n",
    "train_ep_ids = perm[:split]\n",
    "val_ep_ids = perm[split:]\n",
    "\n",
    "train_indices = np.concatenate([episode_start_idx[i] for i in train_ep_ids])\n",
    "val_indices = np.concatenate([episode_start_idx[i] for i in val_ep_ids])\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=256,\n",
    "    sampler=RandomSampler(train_subset, replacement=True),  # 打乱\n",
    "    num_workers=8,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=256,\n",
    "    sampler=SequentialSampler(val_subset),  # 不打乱，便于复现\n",
    "    num_workers=8,\n",
    "    prefetch_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82427400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdejayvu\u001b[0m (\u001b[33mdejayvu-university-of-oxford\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jay/workspace/circle/wandb/run-20250712_011100-wa4s63ly</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model/runs/wa4s63ly' target=\"_blank\">SpaceInvaders-200k</a></strong> to <a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model' target=\"_blank\">https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model/runs/wa4s63ly' target=\"_blank\">https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model/runs/wa4s63ly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 692/692 [12:56<00:00,  1.12s/it, loss=0.442, obs_loss=0.433, reward_loss=0.00847]\n",
      "Validation 01: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 02: 100%|██████████| 692/692 [12:56<00:00,  1.12s/it, loss=0.272, obs_loss=0.265, reward_loss=0.00749]\n",
      "Validation 02: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 03: 100%|██████████| 692/692 [12:56<00:00,  1.12s/it, loss=0.189, obs_loss=0.182, reward_loss=0.00735]\n",
      "Validation 03: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 04: 100%|██████████| 692/692 [12:57<00:00,  1.12s/it, loss=0.146, obs_loss=0.138, reward_loss=0.00743]\n",
      "Validation 04: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 05: 100%|██████████| 692/692 [12:57<00:00,  1.12s/it, loss=0.145, obs_loss=0.138, reward_loss=0.0068] \n",
      "Validation 05: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 06: 100%|██████████| 692/692 [12:57<00:00,  1.12s/it, loss=0.126, obs_loss=0.121, reward_loss=0.00506]\n",
      "Validation 06: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 07: 100%|██████████| 692/692 [12:56<00:00,  1.12s/it, loss=0.116, obs_loss=0.11, reward_loss=0.00605] \n",
      "Validation 07: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 08: 100%|██████████| 692/692 [12:57<00:00,  1.12s/it, loss=0.118, obs_loss=0.114, reward_loss=0.00444]\n",
      "Validation 08: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 09: 100%|██████████| 692/692 [12:57<00:00,  1.12s/it, loss=0.133, obs_loss=0.126, reward_loss=0.00698]\n",
      "Validation 09: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n",
      "Epoch 10: 100%|██████████| 692/692 [12:57<00:00,  1.12s/it, loss=0.143, obs_loss=0.137, reward_loss=0.00641]\n",
      "Validation 10: 100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/obs_loss</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/reward_loss</td><td>▆▅▇▅▅█▄▅▇▅▅█▃▅▇▄▇▆▂▆▅▅▃▅▅▄▅▂▅▅▃▃▂▁▅▁▃▅▄▃</td></tr><tr><td>train/total_loss</td><td>█▅▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/f1</td><td>▁▁▄▅▆▆▇███</td></tr><tr><td>val/obs_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val/precision</td><td>▃█▅▆▄▃▄▃▅▁</td></tr><tr><td>val/recall</td><td>▂▁▃▄▅▆▆▇▇█</td></tr><tr><td>val/reward_loss</td><td>█▇▅▄▂▂▂▂▁▂</td></tr><tr><td>val/total_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/obs_loss</td><td>0.13655</td></tr><tr><td>train/reward_loss</td><td>0.00641</td></tr><tr><td>train/total_loss</td><td>0.14296</td></tr><tr><td>val/f1</td><td>0.40732</td></tr><tr><td>val/obs_loss</td><td>0.17235</td></tr><tr><td>val/precision</td><td>0.28091</td></tr><tr><td>val/recall</td><td>0.74063</td></tr><tr><td>val/reward_loss</td><td>0.00745</td></tr><tr><td>val/total_loss</td><td>0.17979</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SpaceInvaders-200k</strong> at: <a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model/runs/wa4s63ly' target=\"_blank\">https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model/runs/wa4s63ly</a><br> View project at: <a href='https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model' target=\"_blank\">https://wandb.ai/dejayvu-university-of-oxford/pretrain-world-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250712_011100-wa4s63ly/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"pretrain-world-model\",\n",
    "    name=\"SpaceInvaders-200k\",\n",
    "    config={\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 10,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"horizon\": 8,\n",
    "    },\n",
    ")\n",
    "\n",
    "# wandb.watch(world_model, log=\"gradients\", log_freq=100)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(10):\n",
    "    bar = tqdm(train_loader, leave=True, desc=f\"Epoch {epoch + 1:02d}\")\n",
    "    world_model.train()\n",
    "    for frames, actions, rewards in bar:\n",
    "        global_step += 1\n",
    "\n",
    "        B, H, C, Ht, Wt = frames.shape  # (B, H, 3, 84, 84)\n",
    "\n",
    "        frames = frames.view(-1, 3, 84, 84).to(\n",
    "            \"cuda\", dtype=torch.float32, non_blocking=True\n",
    "        )  # (B*H, 3, 84, 84)\n",
    "        actions = actions.to(\n",
    "            \"cuda\", dtype=torch.long, non_blocking=True\n",
    "        )  # (B, H)\n",
    "        rewards = rewards.to(\n",
    "            \"cuda\", dtype=torch.float32, non_blocking=True\n",
    "        )  # (B, H)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z_e = encoder(frames)\n",
    "            _, indices, _ = quantizer(z_e)\n",
    "            obs_tokens = indices.view(B, H, 16)\n",
    "\n",
    "        pred_obs_logits, pred_rewards = world_model(\n",
    "            obs_tokens, actions\n",
    "        )  # (B, H, 16, 512), (B, H)\n",
    "\n",
    "        # get logits ready for loss computation\n",
    "        pred_obs_logits = pred_obs_logits.permute(\n",
    "            0, 1, 3, 2\n",
    "        )  # (B, H, 512, 16)\n",
    "\n",
    "        obs_loss = F.cross_entropy(\n",
    "            pred_obs_logits.reshape(-1, 512),\n",
    "            obs_tokens.reshape(-1),\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "        reward_target = (rewards.abs() > 1e-6).float()\n",
    "        reward_loss = sigmoid_focal_loss(\n",
    "            pred_rewards,\n",
    "            reward_target,\n",
    "            reduction=\"mean\",\n",
    "            alpha=0.8,\n",
    "            gamma=4.5,\n",
    "        )\n",
    "\n",
    "        # total loss\n",
    "        loss = obs_loss + reward_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log to wandb\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/obs_loss\": obs_loss.item(),\n",
    "                \"train/reward_loss\": reward_loss.item(),\n",
    "                \"train/total_loss\": loss.item(),\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "\n",
    "        bar.set_postfix(\n",
    "            obs_loss=obs_loss.item(),\n",
    "            reward_loss=reward_loss.item(),\n",
    "            loss=loss.item(),\n",
    "        )\n",
    "\n",
    "    # Validation loop\n",
    "    val_bar = tqdm(val_loader, leave=True, desc=f\"Validation {epoch + 1:02d}\")\n",
    "    world_model.eval()\n",
    "    obs_loss_sum = 0.0  # 加权和（按 token 计）\n",
    "    reward_loss_sum = 0.0\n",
    "    token_cnt = 0  # obs token 总数 (= B*T*K)\n",
    "\n",
    "    tp = pos = fp = 0  # 召回 / 精度用\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, actions, rewards in val_bar:\n",
    "            B, H, C, Ht, Wt = frames.shape\n",
    "\n",
    "            # ---- 同你原来的前向部分 -----------------------------------------\n",
    "            frames = frames.view(-1, 3, 84, 84).to(\n",
    "                \"cuda\", dtype=torch.float32, non_blocking=True\n",
    "            )\n",
    "            actions = actions.to(\"cuda\", dtype=torch.long, non_blocking=True)\n",
    "            rewards = rewards.to(\n",
    "                \"cuda\", dtype=torch.float32, non_blocking=True\n",
    "            )\n",
    "\n",
    "            z_e = encoder(frames)\n",
    "            _, indices, _ = quantizer(z_e)\n",
    "            obs_tokens = indices.view(B, H, 16)\n",
    "\n",
    "            pred_obs_logits, pred_rewards = world_model(obs_tokens, actions)\n",
    "            pred_obs_logits = pred_obs_logits.permute(0, 1, 3, 2)\n",
    "\n",
    "            # ---- 损失 --------------------------------------------------------\n",
    "            obs_loss = F.cross_entropy(\n",
    "                pred_obs_logits.reshape(-1, 512),\n",
    "                obs_tokens.reshape(-1),\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "            reward_target = (rewards.abs() > 1e-6).float()\n",
    "            reward_loss = sigmoid_focal_loss(\n",
    "                pred_rewards,\n",
    "                reward_target,\n",
    "                reduction=\"mean\",\n",
    "                alpha=0.8,\n",
    "                gamma=4.5,\n",
    "            )\n",
    "\n",
    "            # ---- 加权累加（注意权重是样本/token 数） --------------------------\n",
    "            batch_tokens = obs_tokens.numel()  # B*H*16\n",
    "            obs_loss_sum += obs_loss.item() * batch_tokens\n",
    "            reward_loss_sum += reward_loss.item() * batch_tokens\n",
    "            token_cnt += batch_tokens\n",
    "\n",
    "            # ---- 累加召回 / 精度 --------------------------------------------\n",
    "            prob = torch.sigmoid(pred_rewards)\n",
    "            pred = prob > 0.5\n",
    "\n",
    "            tp += (pred & reward_target.bool()).sum().item()\n",
    "            pos += reward_target.sum().item()\n",
    "            fp += (pred & (~reward_target.bool())).sum().item()\n",
    "\n",
    "    # ─── 2. 循环结束后统一计算 epoch 级指标 ─────────────────────────────────────\n",
    "    obs_loss_epoch = obs_loss_sum / token_cnt\n",
    "    reward_loss_epoch = reward_loss_sum / token_cnt\n",
    "    total_loss_epoch = obs_loss_epoch + reward_loss_epoch\n",
    "\n",
    "    recall = tp / pos if pos else float(\"nan\")\n",
    "    precision = tp / (tp + fp) if (tp + fp) else float(\"nan\")\n",
    "    f1 = (\n",
    "        2 * precision * recall / (precision + recall)\n",
    "        if precision and recall\n",
    "        else float(\"nan\")\n",
    "    )\n",
    "\n",
    "    val_bar.set_postfix(\n",
    "        obs_loss=obs_loss.item(),\n",
    "        reward_loss=reward_loss.item(),\n",
    "        total_loss=total_loss_epoch,\n",
    "        recall=recall,\n",
    "        precision=precision,\n",
    "        f1=f1,\n",
    "    )\n",
    "\n",
    "    # ─── 3. 只 log 一次 ───────────────────────────────────────────────────────\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"val/obs_loss\": obs_loss_epoch,\n",
    "            \"val/reward_loss\": reward_loss_epoch,\n",
    "            \"val/total_loss\": total_loss_epoch,\n",
    "            \"val/recall\": recall,\n",
    "            \"val/precision\": precision,\n",
    "            \"val/f1\": f1,\n",
    "        },\n",
    "        step=global_step,  # 用同一个 global_step 标记这一轮验证\n",
    "    )\n",
    "\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
